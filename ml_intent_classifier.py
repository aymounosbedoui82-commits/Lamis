# ml_intent_classifier.py
"""
Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Machine Learning
âœ… ØªØµÙ†ÙŠÙ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Keywords
âœ… ÙŠØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
âœ… ÙŠØ¯Ø¹Ù… 3 Ù„ØºØ§Øª
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import sqlite3
import json
import re
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import Counter
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


# ==========================================
# 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù„ØºØ§Øª
# ==========================================

class MultilingualTextProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ù†ØµÙˆØµ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª"""
    
    def __init__(self, max_vocab_size: int = 10000, max_seq_length: int = 50):
        self.max_vocab_size = max_vocab_size
        self.max_seq_length = max_seq_length
        
        # Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
        self.word2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}
        self.idx2word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}
        self.word_counts = Counter()
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªÙ†Ø¸ÙŠÙ
        self.arabic_pattern = re.compile(r'[\u0600-\u06FF]+')
        self.french_pattern = re.compile(r'[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§]+')
        self.english_pattern = re.compile(r'[a-zA-Z]+')
        self.number_pattern = re.compile(r'\d+')
        
        # Stop words Ø¨Ø³ÙŠØ·Ø©
        self.stop_words = {
            'ar': {'ÙÙŠ', 'Ù…Ù†', 'Ø¥Ù„Ù‰', 'Ø¹Ù„Ù‰', 'Ø¹Ù†', 'Ù…Ø¹', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'Ùˆ', 'Ø£Ùˆ', 'Ø«Ù…'},
            'fr': {'le', 'la', 'les', 'de', 'du', 'Ã ', 'au', 'en', 'et', 'ou', 'un', 'une'},
            'en': {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'and', 'or', 'is', 'are'}
        }
    
    def detect_language(self, text: str) -> str:
        """ÙƒØ´Ù Ù„ØºØ© Ø§Ù„Ù†Øµ"""
        arabic_chars = len(self.arabic_pattern.findall(text))
        latin_chars = len(self.french_pattern.findall(text))
        
        if arabic_chars > latin_chars:
            return 'ar'
        
        # Ø§Ù„ØªÙØ±ÙŠÙ‚ Ø¨ÙŠÙ† Ø§Ù„ÙØ±Ù†Ø³ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        french_indicators = ['je', 'tu', 'il', 'nous', 'vous', 'rdv', 'rendez', 'demain', 'aujourd']
        if any(ind in text.lower() for ind in french_indicators):
            return 'fr'
        
        return 'en'
    
    def normalize_arabic(self, text: str) -> str:
        """ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‡Ù…Ø²Ø§Øª
        text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
        text = re.sub(r'[Ø¤Ø¦]', 'Ø¡', text)
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[\u064B-\u065F]', '', text)
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø© ÙˆØ§Ù„Ù‡Ø§Ø¡
        text = re.sub(r'Ø©', 'Ù‡', text)
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª"""
        text = text.lower()
        language = self.detect_language(text)
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        if language == 'ar':
            text = self.normalize_arabic(text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        words = re.findall(r'[\u0600-\u06FF]+|[a-zA-ZÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼Ã¿Ã§]+|\d+', text)
        
        # Ø¥Ø²Ø§Ù„Ø© stop words
        stop = self.stop_words.get(language, set())
        words = [w for w in words if w not in stop and len(w) > 1]
        
        return words
    
    def build_vocabulary(self, texts: List[str]):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ"""
        for text in texts:
            tokens = self.tokenize(text)
            self.word_counts.update(tokens)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø´ÙŠÙˆØ¹Ø§Ù‹
        most_common = self.word_counts.most_common(self.max_vocab_size - 4)
        
        for idx, (word, _) in enumerate(most_common, start=4):
            self.word2idx[word] = idx
            self.idx2word[idx] = word
        
        logger.info(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ø¨Ù€ {len(self.word2idx)} ÙƒÙ„Ù…Ø©")
    
    def encode(self, text: str) -> torch.Tensor:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ tensor"""
        tokens = self.tokenize(text)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ indices
        indices = [self.word2idx.get(token, 1) for token in tokens]  # 1 = <UNK>
        
        # Padding Ø£Ùˆ Truncation
        if len(indices) < self.max_seq_length:
            indices = indices + [0] * (self.max_seq_length - len(indices))
        else:
            indices = indices[:self.max_seq_length]
        
        return torch.tensor(indices, dtype=torch.long)
    
    def save(self, path: str):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬"""
        data = {
            'word2idx': self.word2idx,
            'idx2word': self.idx2word,
            'max_seq_length': self.max_seq_length
        }
        with open(path, 'wb') as f:
            pickle.dump(data, f)
    
    def load(self, path: str):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        self.word2idx = data['word2idx']
        self.idx2word = data['idx2word']
        self.max_seq_length = data['max_seq_length']


# ==========================================
# 2. Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§
# ==========================================

class IntentClassifierLSTM(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ LSTM Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§"""
    
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 128,
        hidden_dim: int = 256,
        num_layers: int = 2,
        num_intents: int = 10,
        dropout: float = 0.3
    ):
        super(IntentClassifierLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_dim * 2, 1)
        
        # Fully connected layers
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_intents)
        
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)
    
    def attention_weights(self, lstm_output: torch.Tensor) -> torch.Tensor:
        """Ø­Ø³Ø§Ø¨ Ø£ÙˆØ²Ø§Ù† Attention"""
        # lstm_output: (batch, seq_len, hidden*2)
        attention_scores = self.attention(lstm_output)  # (batch, seq_len, 1)
        attention_weights = F.softmax(attention_scores, dim=1)
        return attention_weights
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass
        
        Returns:
            logits: (batch, num_intents)
            attention_weights: (batch, seq_len, 1)
        """
        # Embedding
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        embedded = self.dropout(embedded)
        
        # LSTM
        lstm_out, _ = self.lstm(embedded)  # (batch, seq_len, hidden*2)
        
        # Attention
        attn_weights = self.attention_weights(lstm_out)
        
        # Weighted sum
        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch, hidden*2)
        
        # Classification
        x = self.dropout(context)
        x = F.relu(self.fc1(x))
        x = self.layer_norm(x)
        x = self.dropout(x)
        logits = self.fc2(x)
        
        return logits, attn_weights


class IntentClassifierCNN(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ CNN Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ (Ø£Ø³Ø±Ø¹)"""
    
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 128,
        num_filters: int = 100,
        filter_sizes: List[int] = [2, 3, 4, 5],
        num_intents: int = 10,
        dropout: float = 0.5
    ):
        super(IntentClassifierCNN, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # Multiple CNN filters
        self.convs = nn.ModuleList([
            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)
            for fs in filter_sizes
        ])
        
        self.fc = nn.Linear(num_filters * len(filter_sizes), num_intents)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Embedding
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        embedded = embedded.permute(0, 2, 1)  # (batch, embed_dim, seq_len)
        
        # CNN + MaxPool
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(embedded))  # (batch, num_filters, *)
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)
        
        # Concatenate
        concat = torch.cat(conv_outputs, dim=1)  # (batch, num_filters * len(filter_sizes))
        
        # Dropout + FC
        x = self.dropout(concat)
        logits = self.fc(x)
        
        return logits


# ==========================================
# 3. Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================

class IntentDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ÙˆØ§ÙŠØ§"""
    
    INTENT_LABELS = [
        'add_appointment',      # Ø¥Ø¶Ø§ÙØ© Ù…ÙˆØ¹Ø¯
        'list_appointments',    # Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯
        'check_specific_day',   # Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙŠÙˆÙ… Ù…Ø­Ø¯Ø¯
        'cancel_appointment',   # Ø¥Ù„ØºØ§Ø¡ Ù…ÙˆØ¹Ø¯
        'modify_appointment',   # ØªØ¹Ø¯ÙŠÙ„ Ù…ÙˆØ¹Ø¯
        'set_reminder',         # ØªØ¹ÙŠÙŠÙ† ØªØ°ÙƒÙŠØ±
        'greeting',             # ØªØ­ÙŠØ©
        'thanks',               # Ø´ÙƒØ±
        'help',                 # Ù…Ø³Ø§Ø¹Ø¯Ø©
        'unknown'               # ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ
    ]
    
    def __init__(
        self,
        db_path: str = "agent_data.db",
        processor: MultilingualTextProcessor = None,
        augment: bool = True
    ):
        self.db_path = db_path
        self.processor = processor or MultilingualTextProcessor()
        self.augment = augment
        
        self.samples = []
        self.labels = []
        
        self._load_from_database()
        self._add_synthetic_data()
        
        if not self.processor.word2idx or len(self.processor.word2idx) <= 4:
            texts = [s for s, _ in self.samples]
            self.processor.build_vocabulary(texts)
    
    def _load_from_database(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT user_message, intent, feedback
                FROM interactions
                WHERE intent IS NOT NULL AND intent != ''
                ORDER BY timestamp DESC
                LIMIT 50000
            ''')
            
            for row in cursor.fetchall():
                message, intent, feedback = row
                if intent in self.INTENT_LABELS:
                    # Ø¥Ø¹Ø·Ø§Ø¡ ÙˆØ²Ù† Ø£ÙƒØ¨Ø± Ù„Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø°Ø§Øª feedback Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
                    weight = 1 + (feedback or 0) * 0.2
                    for _ in range(int(weight)):
                        self.samples.append((message, intent))
            
            conn.close()
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.samples)} ØªÙØ§Ø¹Ù„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def _add_synthetic_data(self):
        """Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ØµÙ†Ø§Ø¹ÙŠØ© Ù…ÙˆØ³Ø¹Ø©"""
        synthetic_data = {
            'add_appointment': [
                # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ø§Ù‹
                "Ù…ÙˆØ¹Ø¯ ØºØ¯Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 3",
                "Ø£Ø±ÙŠØ¯ Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯",
                "Ø³Ø¬Ù„ Ù„ÙŠ Ù…ÙˆØ¹Ø¯ Ù…Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¨",
                "Ø¹Ù†Ø¯ÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¨ÙƒØ±Ø©",
                "Ù…ÙˆØ¹Ø¯ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³ Ø§Ù„Ø³Ø§Ø¹Ø© 10",
                "Ù„Ø¯ÙŠ Ù„Ù‚Ø§Ø¡ Ù…Ù‡Ù… ØºØ¯Ø§Ù‹",
                "Ø£Ø¶Ù Ù…ÙˆØ¹Ø¯ Ø¬Ø¯ÙŠØ¯",
                "Ø³Ø¬Ù„ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø© 5 Ù…Ø³Ø§Ø¡",
                "Ù…ÙˆØ¹Ø¯ Ù…Ø¹ Ø§Ù„Ù…Ø¯ÙŠØ± ØºØ¯Ø§Ù‹ ØµØ¨Ø§Ø­Ø§Ù‹",
                "Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯ Ù„Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…",
                "Ù…ÙˆØ¹Ø¯",
                "Ø§Ø¬ØªÙ…Ø§Ø¹",
                "Ù„Ù‚Ø§Ø¡",
                "Ù…Ù‚Ø§Ø¨Ù„Ø©",
                "Ø£Ø±ÙŠØ¯ Ù…ÙˆØ¹Ø¯",
                "Ø§Ø­ØªØ§Ø¬ Ù…ÙˆØ¹Ø¯",
                "Ø³Ø¬Ù„ Ù…ÙˆØ¹Ø¯",
                "Ø£Ø¶Ù Ø§Ø¬ØªÙ…Ø§Ø¹",
                "Ù…ÙˆØ¹Ø¯ Ø¬Ø¯ÙŠØ¯",
                "Ø­Ø¬Ø² Ø¬Ø¯ÙŠØ¯",
                "Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø© 4",
                "Ø§Ø¬ØªÙ…Ø§Ø¹ Ø§Ù„Ø³Ø§Ø¹Ø© 2",
                "Ù…ÙˆØ¹Ø¯ ØµØ¨Ø§Ø­Ø§Ù‹",
                "Ù…ÙˆØ¹Ø¯ Ù…Ø³Ø§Ø¡Ù‹",
                "Ù…ÙˆØ¹Ø¯ Ø¨Ø¹Ø¯ Ø§Ù„Ø¸Ù‡Ø±",
                "Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¹Ù…Ù„",
                "Ù„Ù‚Ø§Ø¡ Ø¹Ù…Ù„",
                "Ù…ÙˆØ¹Ø¯ Ø·Ø¨ÙŠØ¨",
                "Ù…ÙˆØ¹Ø¯ Ø¯ÙƒØªÙˆØ±",
                "Ù…ÙˆØ¹Ø¯ Ù…Ø³ØªØ´ÙÙ‰",
                "Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚",
                "Ù„Ù‚Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ø¹Ù…ÙŠÙ„",
                # Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©
                "Ø¹Ù†Ø¯ÙŠ Ø±Ù†Ø¯ÙŠ ÙÙˆ ØºØ¯ÙˆØ©",
                "Ù†Ø­Ø¨ Ù†Ø³Ø¬Ù„ Ù…ÙˆØ¹Ø¯",
                "Ø±Ù†Ø¯ÙŠ ÙÙˆ",
                "Ø¹Ù†Ø¯ÙŠ Ø±Ù†Ø¯ÙŠ ÙÙˆ",
                "Ù†Ø­Ø¨ Ù†Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯",
                # Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©
                "RDV demain Ã  15h",
                "Je voudrais prendre rendez-vous",
                "RÃ©union lundi matin",
                "Ajouter un rendez-vous",
                "RDV mÃ©decin demain",
                "Planifier une rÃ©union",
                "rdv",
                "rendez-vous",
                "prendre rdv",
                "nouveau rdv",
                "ajouter rdv",
                "rÃ©union",
                "rdv Ã  14h",
                "rdv Ã  15h",
                "rdv demain",
                "rdv lundi",
                "je veux un rdv",
                "je voudrais un rdv",
                # Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
                "Appointment tomorrow at 3pm",
                "Schedule a meeting",
                "Book an appointment",
                "I have a meeting tomorrow",
                "Set up appointment for Monday",
                "appointment",
                "meeting",
                "schedule",
                "book",
                "new appointment",
                "add meeting",
                "create appointment",
                "i need an appointment",
                "i want to schedule",
                "meeting at 3",
                "appointment at 2pm",
            ],
            'list_appointments': [
                "Ø¹Ø±Ø¶ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ø£Ø¸Ù‡Ø± Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "ÙƒÙ„ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ø´ÙˆÙ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ø§Ø¹Ø±Ø¶ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Ø¹Ø±Ø¶",
                "Ø£Ø¸Ù‡Ø±",
                "Ù‚Ø§Ø¦Ù…Ø©",
                "Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Ø¬Ù…ÙŠØ¹ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "ÙƒÙ„ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Ø£Ø±Ù†ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "ÙˆØ±ÙŠÙ†ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ø´Ùˆ Ø¹Ù†Ø¯ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Afficher mes RDV",
                "Mes rendez-vous",
                "Voir mes RDV",
                "mes rdv",
                "afficher rdv",
                "voir rdv",
                "liste rdv",
                "tous mes rdv",
                "montrer mes rdv",
                "Show my appointments",
                "List all appointments",
                "My appointments",
                "What are my appointments",
                "show appointments",
                "list appointments",
                "my schedule",
                "view appointments",
                "see my appointments",
                "all my appointments",
            ],
            'check_specific_day': [
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ Ø§Ù„ÙŠÙˆÙ…",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ ØºØ¯Ø§Ù‹",
                "Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³",
                "Ù…Ø§Ø°Ø§ Ù„Ø¯ÙŠ ØºØ¯Ø§Ù‹",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ ÙŠÙˆÙ… 25 Ù…Ø§Ø±Ø³",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ ØºØ¯Ø§Ù‹",
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø¨ÙƒØ±Ø©",
                "Ù…Ø§Ø°Ø§ Ø¹Ù†Ø¯ÙŠ Ø§Ù„ÙŠÙˆÙ…",
                "Ø´Ùˆ Ø¹Ù†Ø¯ÙŠ Ø§Ù„ÙŠÙˆÙ…",
                "Ø§ÙŠØ´ Ø¹Ù†Ø¯ÙŠ Ø¨ÙƒØ±Ø©",
                "Mes RDV aujourd'hui",
                "RDV de demain",
                "Mes rendez-vous du lundi",
                "rdv aujourd'hui",
                "rdv demain",
                "mes rdv de demain",
                "Today's appointments",
                "What do I have tomorrow",
                "Appointments on Monday",
                "today appointments",
                "tomorrow schedule",
                "what's on my schedule today",
                "appointments today",
                "appointments tomorrow",
            ],
            'cancel_appointment': [
                "Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ø§Ø­Ø°Ù Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ø£Ù„ØºÙŠ Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¨",
                "Ø­Ø°Ù Ø§Ù„Ù…ÙˆØ¹Ø¯ Ø±Ù‚Ù… 5",
                "Ø¥Ù„ØºØ§Ø¡ Ù…ÙˆØ¹Ø¯ Ø§Ù„ØºØ¯",
                "Ø¥Ù„ØºØ§Ø¡",
                "Ø§Ù„ØºØ§Ø¡",
                "Ø­Ø°Ù",
                "Ø§Ù…Ø³Ø­",
                "Ø´ÙŠÙ„",
                "Ø£Ù„ØºÙŠ",
                "Ø§Ø­Ø°Ù",
                "Ø§Ù…Ø­ÙŠ",
                "Ø¥Ù„ØºØ§Ø¡ Ù…ÙˆØ¹Ø¯",
                "Ø­Ø°Ù Ù…ÙˆØ¹Ø¯",
                "Ø£Ù„ØºÙŠ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ù„Ø§ Ø£Ø±ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Annuler le RDV",
                "Supprimer rendez-vous",
                "annuler",
                "supprimer",
                "annuler rdv",
                "supprimer rdv",
                "Cancel the appointment",
                "Delete appointment",
                "Remove meeting",
                "cancel",
                "delete",
                "remove",
                "cancel appointment",
                "delete meeting",
                "remove appointment",
            ],
            'modify_appointment': [
                "ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØºÙŠØ± Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¨",
                "ØªØ£Ø¬ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØªØºÙŠÙŠØ± ÙˆÙ‚Øª Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØªØ¹Ø¯ÙŠÙ„",
                "ØªØºÙŠÙŠØ±",
                "ØªØ£Ø¬ÙŠÙ„",
                "ØªÙ‚Ø¯ÙŠÙ…",
                "ØºÙŠØ±",
                "Ø¹Ø¯Ù„",
                "Ø¨Ø¯Ù„",
                "ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ø¹Ø¯Ù„ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Modifier le RDV",
                "Changer l'heure",
                "modifier",
                "changer",
                "reporter",
                "modifier rdv",
                "changer rdv",
                "Change appointment time",
                "Reschedule meeting",
                "Update appointment",
                "change",
                "modify",
                "reschedule",
                "update",
                "change appointment",
                "modify meeting",
            ],
            'set_reminder': [
                "Ø°ÙƒØ±Ù†ÙŠ Ù‚Ø¨Ù„ 30 Ø¯Ù‚ÙŠÙ‚Ø©",
                "Ø°ÙƒØ±Ù†ÙŠ Ø¨Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Ø£Ø±ÙŠØ¯ ØªØ°ÙƒÙŠØ±",
                "ØªØ°ÙƒÙŠØ± Ù‚Ø¨Ù„ Ø³Ø§Ø¹Ø©",
                "Ø°ÙƒØ±Ù†ÙŠ",
                "ØªØ°ÙƒÙŠØ±",
                "ÙØ¹Ù„ Ø§Ù„ØªØ°ÙƒÙŠØ±",
                "Ø£Ø±ÙŠØ¯ ØªÙ†Ø¨ÙŠÙ‡",
                "Ù†Ø¨Ù‡Ù†ÙŠ",
                "Rappelle-moi avant",
                "Mettre un rappel",
                "rappel",
                "rappelle-moi",
                "Remind me before",
                "Set a reminder",
                "reminder",
                "remind me",
                "set reminder",
                "notify me",
            ],
            'greeting': [
                "Ù…Ø±Ø­Ø¨Ø§",
                "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…",
                "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±",
                "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±",
                "Ø£Ù‡Ù„Ø§",
                "Ù‡Ø§ÙŠ",
                "Ù‡Ù„Ø§",
                "Ø§Ù‡Ù„ÙŠÙ†",
                "Ø³Ù„Ø§Ù…",
                "Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙÙƒ",
                "ÙƒÙŠÙ Ø§Ù„Ø­Ø§Ù„",
                "Ø´Ù„ÙˆÙ†Ùƒ",
                "ÙƒÙŠÙÙƒ",
                "Ø£Ù‡Ù„Ø§ ÙˆØ³Ù‡Ù„Ø§",
                "ÙŠØ§ Ù‡Ù„Ø§",
                "Bonjour",
                "Salut",
                "Bonsoir",
                "Coucou",
                "Bonne journÃ©e",
                "Hello",
                "Hi",
                "Hey",
                "Good morning",
                "Good evening",
                "Good afternoon",
                "Hi there",
                "Hello there",
                "Howdy",
            ],
            'thanks': [
                "Ø´ÙƒØ±Ø§",
                "Ø´ÙƒØ±Ø§Ù‹ Ø¬Ø²ÙŠÙ„Ø§Ù‹",
                "Ù…Ø´ÙƒÙˆØ±",
                "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©",
                "ØªØ³Ù„Ù…",
                "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©",
                "Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±",
                "Ù…Ù…Ù†ÙˆÙ†",
                "Ø´ÙƒØ±Ø§ Ù„Ùƒ",
                "Merci",
                "Merci beaucoup",
                "Merci bien",
                "Thanks",
                "Thank you",
                "Thank you so much",
                "Thanks a lot",
                "Many thanks",
                "Appreciated",
            ],
            'help': [
                "Ù…Ø³Ø§Ø¹Ø¯Ø©",
                "Ø³Ø§Ø¹Ø¯Ù†ÙŠ",
                "ÙƒÙŠÙ Ø£Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙˆØª",
                "Ù…Ø§Ø°Ø§ ÙŠÙ…ÙƒÙ†Ùƒ ÙØ¹Ù„Ù‡",
                "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©",
                "Ø£Ø­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø©",
                "ÙƒÙŠÙ",
                "Ø´Ù„ÙˆÙ†",
                "ÙƒÙŠÙ Ø£Ø³ÙˆÙŠ",
                "ÙƒÙŠÙ Ø£Ø¹Ù…Ù„",
                "Ø§Ù„Ø£ÙˆØ§Ù…Ø±",
                "Ù…Ø§Ø°Ø§ ØªÙØ¹Ù„",
                "Ø´Ùˆ Ø¨ØªØ¹Ø±Ù ØªØ³ÙˆÙŠ",
                "Ø§ÙŠØ´ ØªÙ‚Ø¯Ø± ØªØ³ÙˆÙŠ",
                "Aide",
                "Comment utiliser",
                "aide-moi",
                "comment faire",
                "qu'est-ce que tu fais",
                "Help",
                "How to use",
                "What can you do",
                "help me",
                "how do i",
                "instructions",
                "commands",
                "what do you do",
            ],
        }
        
        for intent, examples in synthetic_data.items():
            for example in examples:
                self.samples.append((example, intent))
                
                # Data augmentation - Ø¥Ø¶Ø§ÙØ© Ù†Ø³Ø® Ù…ØªØ¹Ø¯Ø¯Ø©
                if self.augment:
                    # Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø·ÙÙŠÙØ©
                    for _ in range(3):  # 3 Ù†Ø³Ø® Ø¥Ø¶Ø§ÙÙŠØ© Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„
                        augmented = self._augment_text(example)
                        self.samples.append((augmented, intent))
        
        logger.info(f"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {len(self.samples)}")
    
    def _augment_text(self, text: str) -> str:
        """ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØªÙ†ÙˆØ¹ Ø£ÙƒØ¨Ø±"""
        import random
        
        augmentations = [
            lambda t: t.lower(),
            lambda t: t.upper(),
            lambda t: t.capitalize(),
            lambda t: t + ".",
            lambda t: t + "ØŸ" if any(c in t for c in 'Ø¡Ø£Ø¥Ø¢Ø¤Ø¦') else t + "?",
            lambda t: t + "!",
            lambda t: " ".join(t.split()),  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            lambda t: t.strip(),
            lambda t: "  " + t,  # Ù…Ø³Ø§ÙØ§Øª ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
            lambda t: t + "  ",  # Ù…Ø³Ø§ÙØ§Øª ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
            lambda t: t.replace("Ø§", "Ø£") if "Ø§" in t else t,
            lambda t: t.replace("Ø£", "Ø§") if "Ø£" in t else t,
            lambda t: t.replace("Ø©", "Ù‡") if "Ø©" in t else t,
        ]
        
        # Ø§Ø®ØªÙŠØ§Ø± 1-2 ØªØ­ÙˆÙŠÙ„Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        num_augs = random.randint(1, 2)
        result = text
        for _ in range(num_augs):
            aug_func = random.choice(augmentations)
            result = aug_func(result)
        
        return result
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        text, intent = self.samples[idx]
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ
        encoded = self.processor.encode(text)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ÙŠØ© Ø¥Ù„Ù‰ Ø±Ù‚Ù…
        label = self.INTENT_LABELS.index(intent) if intent in self.INTENT_LABELS else len(self.INTENT_LABELS) - 1
        
        return encoded, torch.tensor(label, dtype=torch.long)


# ==========================================
# 4. Ø§Ù„Ù…ØµÙ†Ù Ø§Ù„Ø°ÙƒÙŠ
# ==========================================

class SmartIntentClassifier:
    """Ù…ØµÙ†Ù Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(
        self,
        model_path: str = "models/intent_classifier.pth",
        processor_path: str = "models/text_processor.pkl",
        db_path: str = "agent_data.db",
        model_type: str = "lstm"  # "lstm" or "cnn"
    ):
        self.model_path = model_path
        self.processor_path = processor_path
        self.db_path = db_path
        self.model_type = model_type
        
        self.processor = MultilingualTextProcessor()
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.intent_labels = IntentDataset.INTENT_LABELS
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…ÙˆØ¬ÙˆØ¯
        self._load_model()
    
    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸"""
        try:
            if Path(self.processor_path).exists():
                self.processor.load(self.processor_path)
                logger.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ")
            
            if Path(self.model_path).exists():
                checkpoint = torch.load(self.model_path, map_location=self.device)
                
                vocab_size = len(self.processor.word2idx)
                num_intents = len(self.intent_labels)
                
                if self.model_type == "lstm":
                    self.model = IntentClassifierLSTM(vocab_size, num_intents=num_intents)
                else:
                    self.model = IntentClassifierCNN(vocab_size, num_intents=num_intents)
                
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.model.to(self.device)
                self.model.eval()
                
                logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† {self.model_path}")
            else:
                logger.info("â„¹ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ - Ø³ÙŠØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ù„Ø¨")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    
    def train(
        self,
        epochs: int = 50,
        batch_size: int = 16,
        learning_rate: float = 0.002,
        validation_split: float = 0.15
    ) -> Dict:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        print("\n" + "="*70)
        print("ğŸ§  Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§")
        print("="*70)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        dataset = IntentDataset(self.db_path, self.processor)
        
        if len(dataset) < 50:
            print(f"\nâŒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©: {len(dataset)} Ø¹ÙŠÙ†Ø© (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰: 50)")
            return {'success': False, 'reason': 'insufficient_data'}
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        train_size = int((1 - validation_split) * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)
        
        print(f"\nğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        print(f"   â€¢ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_size} Ø¹ÙŠÙ†Ø©")
        print(f"   â€¢ Ø§Ù„ØªØ­Ù‚Ù‚: {val_size} Ø¹ÙŠÙ†Ø©")
        print(f"   â€¢ Ø§Ù„Ù†ÙˆØ§ÙŠØ§: {len(self.intent_labels)}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        vocab_size = len(self.processor.word2idx)
        
        if self.model_type == "lstm":
            self.model = IntentClassifierLSTM(vocab_size, num_intents=len(self.intent_labels))
        else:
            self.model = IntentClassifierCNN(vocab_size, num_intents=len(self.intent_labels))
        
        self.model.to(self.device)
        
        # Optimizer Ùˆ Loss
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)
        criterion = nn.CrossEntropyLoss()
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
        best_val_acc = 0
        
        print(f"\n{'â”€'*70}")
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                
                if self.model_type == "lstm":
                    outputs, _ = self.model(batch_x)
                else:
                    outputs = self.model(batch_x)
                
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # Validation
            self.model.eval()
            val_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x = batch_x.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    if self.model_type == "lstm":
                        outputs, _ = self.model(batch_x)
                    else:
                        outputs = self.model(batch_x)
                    
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    
                    _, predicted = torch.max(outputs, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            
            val_loss /= len(val_loader)
            val_acc = 100 * correct / total
            
            scheduler.step(val_loss)
            
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø¯Ù…
            print(f"Epoch {epoch+1:3d}/{epochs} â”‚ "
                  f"Train Loss: {train_loss:.4f} â”‚ "
                  f"Val Loss: {val_loss:.4f} â”‚ "
                  f"Val Acc: {val_acc:.1f}%", end="")
            
            # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self._save_model()
                print(" â­ Best!")
            else:
                print()
        
        print(f"{'â”€'*70}")
        print(f"\nğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
        print(f"â­ Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_val_acc:.1f}%")
        
        return {
            'success': True,
            'best_accuracy': best_val_acc,
            'history': history
        }
    
    def _save_model(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        Path(self.model_path).parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_type': self.model_type,
            'intent_labels': self.intent_labels,
            'timestamp': datetime.now().isoformat()
        }, self.model_path)
        
        self.processor.save(self.processor_path)
        logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ {self.model_path}")
    
    def predict(self, text: str) -> Dict:
        """
        Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù†ÙŠØ© Ø§Ù„Ù†Øµ
        
        Returns:
            dict: {
                'intent': Ø§Ù„Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©,
                'confidence': Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©,
                'all_scores': Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬,
                'method': 'ml' Ø£Ùˆ 'fallback'
            }
        """
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†Ù…ÙˆØ°Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯
        if self.model is None:
            return self._rule_based_classify(text)
        
        try:
            self.model.eval()
            
            with torch.no_grad():
                encoded = self.processor.encode(text).unsqueeze(0).to(self.device)
                
                if self.model_type == "lstm":
                    outputs, attention = self.model(encoded)
                else:
                    outputs = self.model(encoded)
                
                probabilities = F.softmax(outputs, dim=1)[0]
                confidence, predicted = torch.max(probabilities, 0)
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ dict
                all_scores = {
                    self.intent_labels[i]: probabilities[i].item()
                    for i in range(len(self.intent_labels))
                }
                
                predicted_intent = self.intent_labels[predicted.item()]
                confidence_score = confidence.item()
                
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙƒÙ€ fallback
                if confidence_score < 0.5:
                    rule_result = self._rule_based_classify(text)
                    if rule_result['confidence'] > confidence_score:
                        return rule_result
                
                return {
                    'intent': predicted_intent,
                    'confidence': confidence_score,
                    'all_scores': all_scores,
                    'method': 'ml'
                }
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return self._rule_based_classify(text)
    
    def _rule_based_classify(self, text: str) -> Dict:
        """ØªØµÙ†ÙŠÙ Ø¨Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (fallback)"""
        text_lower = text.lower()
        
        # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙ
        rules = {
            'add_appointment': [
                'Ù…ÙˆØ¹Ø¯', 'Ø§Ø¬ØªÙ…Ø§Ø¹', 'Ù„Ù‚Ø§Ø¡', 'Ù…Ù‚Ø§Ø¨Ù„Ø©', 'Ø£Ø¶Ù', 'Ø³Ø¬Ù„', 'Ø­Ø¬Ø²',
                'rdv', 'rendez-vous', 'rÃ©union', 'ajouter',
                'appointment', 'meeting', 'schedule', 'book'
            ],
            'list_appointments': [
                'Ø¹Ø±Ø¶ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ', 'Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ', 'Ø£Ø¸Ù‡Ø± Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯', 'ÙƒÙ„ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ',
                'mes rdv', 'mes rendez-vous', 'afficher',
                'my appointments', 'show appointments', 'list'
            ],
            'check_specific_day': [
                'Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ Ø§Ù„ÙŠÙˆÙ…', 'Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ ØºØ¯Ø§', 'Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙŠÙˆÙ…',
                'rdv aujourd', 'rdv demain',
                'today', 'tomorrow', 'appointments on'
            ],
            'cancel_appointment': [
                'Ø¥Ù„ØºØ§Ø¡', 'Ø§Ø­Ø°Ù', 'Ø­Ø°Ù', 'Ø£Ù„ØºÙŠ',
                'annuler', 'supprimer',
                'cancel', 'delete', 'remove'
            ],
            'greeting': [
                'Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø£Ù‡Ù„Ø§',
                'bonjour', 'salut', 'bonsoir',
                'hello', 'hi', 'hey', 'good morning'
            ],
            'thanks': [
                'Ø´ÙƒØ±Ø§', 'Ù…Ø´ÙƒÙˆØ±',
                'merci',
                'thanks', 'thank you'
            ],
            'help': [
                'Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ø³Ø§Ø¹Ø¯Ù†ÙŠ', 'ÙƒÙŠÙ',
                'aide', 'comment',
                'help', 'how'
            ]
        }
        
        best_intent = 'unknown'
        best_score = 0
        
        for intent, keywords in rules.items():
            matches = sum(1 for kw in keywords if kw in text_lower)
            score = matches / len(keywords) if keywords else 0
            
            if score > best_score:
                best_score = score
                best_intent = intent
        
        return {
            'intent': best_intent,
            'confidence': min(best_score * 2, 0.9),  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø«Ù‚Ø©
            'all_scores': {},
            'method': 'rule_based'
        }


# ==========================================
# Ø§Ø®ØªØ¨Ø§Ø±
# ==========================================

if __name__ == "__main__":
    print("="*70)
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§ Ø§Ù„Ø°ÙƒÙŠ")
    print("="*70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ØµÙ†Ù
    classifier = SmartIntentClassifier(model_type="lstm")
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("\nğŸ“š ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    result = classifier.train(epochs=10, batch_size=16)
    
    if result['success']:
        print(f"\nâœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ø§Ø¬Ø­! Ø§Ù„Ø¯Ù‚Ø©: {result['best_accuracy']:.1f}%")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤
        test_messages = [
            "Ù…ÙˆØ¹Ø¯ ØºØ¯Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 3",
            "Ø¹Ø±Ø¶ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
            "Ù…Ø±Ø­Ø¨Ø§",
            "RDV demain Ã  15h",
            "My appointments today",
            "Ø´ÙƒØ±Ø§Ù‹ Ø¬Ø²ÙŠÙ„Ø§Ù‹"
        ]
        
        print("\n" + "â”€"*70)
        print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤:")
        print("â”€"*70)
        
        for msg in test_messages:
            result = classifier.predict(msg)
            print(f"\nğŸ’¬ '{msg}'")
            print(f"   â†’ Ø§Ù„Ù†ÙŠØ©: {result['intent']}")
            print(f"   â†’ Ø§Ù„Ø«Ù‚Ø©: {result['confidence']*100:.1f}%")
            print(f"   â†’ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©: {result['method']}")
    
    print("\n" + "="*70)
    print("âœ… Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù†ØªÙ‡Ù‰!")
