# bert_arabic_classifier.py
"""
Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© - ØªØµÙ†ÙŠÙ Ù†ÙˆØ§ÙŠØ§ Ù…ØªÙ‚Ø¯Ù…
âœ… ÙŠØ³ØªØ®Ø¯Ù… AraBERT Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚
âœ… Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹
âœ… ÙŠÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø¹Ù†Ù‰
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Optional, Tuple
import numpy as np
import json
import sqlite3
from pathlib import Path
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± transformers
try:
    from transformers import AutoTokenizer, AutoModel, AutoConfig
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("âš ï¸ Ù…ÙƒØªØ¨Ø© transformers ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ·")


# ==========================================
# 1. Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„ØªØµÙ†ÙŠÙ
# ==========================================

class ArabicBERTClassifier(nn.Module):
    """Ù…ØµÙ†Ù BERT Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"""
    
    def __init__(
        self,
        model_name: str = "aubmindlab/bert-base-arabertv2",
        num_intents: int = 10,
        dropout: float = 0.3,
        freeze_bert: bool = False
    ):
        super(ArabicBERTClassifier, self).__init__()
        
        self.model_name = model_name
        self.num_intents = num_intents
        
        if TRANSFORMERS_AVAILABLE:
            # ØªØ­Ù…ÙŠÙ„ BERT
            self.bert = AutoModel.from_pretrained(model_name)
            self.config = self.bert.config
            hidden_size = self.config.hidden_size
            
            # ØªØ¬Ù…ÙŠØ¯ Ø·Ø¨Ù‚Ø§Øª BERT (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            if freeze_bert:
                for param in self.bert.parameters():
                    param.requires_grad = False
        else:
            # Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ·
            hidden_size = 256
            self.embedding = nn.Embedding(50000, hidden_size)
            self.lstm = nn.LSTM(hidden_size, hidden_size // 2, bidirectional=True, batch_first=True)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_intents)
        )
    
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:
        if TRANSFORMERS_AVAILABLE:
            # BERT forward
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… [CLS] token
            pooled_output = outputs.last_hidden_state[:, 0, :]
        else:
            # Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„
            embedded = self.embedding(input_ids)
            lstm_out, _ = self.lstm(embedded)
            pooled_output = lstm_out[:, 0, :]
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        
        return logits


# ==========================================
# 2. Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª BERT
# ==========================================

class BERTIntentDataset(Dataset):
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ BERT"""
    
    INTENT_LABELS = [
        'add_appointment',
        'list_appointments',
        'check_specific_day',
        'cancel_appointment',
        'modify_appointment',
        'set_reminder',
        'greeting',
        'thanks',
        'help',
        'unknown'
    ]
    
    def __init__(
        self,
        tokenizer,
        max_length: int = 64,
        db_path: str = "agent_data.db"
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.db_path = db_path
        
        self.samples = []
        self._load_data()
    
    def _load_data(self):
        """ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ØµÙ†Ø§Ø¹ÙŠØ© Ø´Ø§Ù…Ù„Ø©
        training_data = {
            'add_appointment': [
                # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰
                "Ø£Ø±ÙŠØ¯ Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯", "Ù…ÙˆØ¹Ø¯ ØºØ¯Ø§Ù‹", "Ø³Ø¬Ù„ Ù„ÙŠ Ù…ÙˆØ¹Ø¯ Ù…Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¨",
                "Ø£Ø¶Ù Ù…ÙˆØ¹Ø¯ Ø¬Ø¯ÙŠØ¯", "Ù…ÙˆØ¹Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©", "Ù„Ø¯ÙŠ Ø§Ø¬ØªÙ…Ø§Ø¹ ØºØ¯Ø§Ù‹",
                "Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯ Ù„Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ù‚Ø§Ø¯Ù…", "Ù…ÙˆØ¹Ø¯ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³", "Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ù‡Ù… ØºØ¯Ø§Ù‹ ØµØ¨Ø§Ø­Ø§Ù‹",
                "Ø£Ø±ÙŠØ¯ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¹Ø¯", "Ù„Ù‚Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ù…Ø¯ÙŠØ±", "Ù…ÙˆØ¹Ø¯ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© 4",
                "Ø³Ø¬Ù„ Ù…ÙˆØ¹Ø¯ Ø¹Ù†Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¨", "Ù…ÙˆØ¹Ø¯ ÙØ­Øµ Ø·Ø¨ÙŠ", "Ø§Ø¬ØªÙ…Ø§Ø¹ Ø¹Ù…Ù„",
                # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ù…ÙŠØ©/Ø§Ù„ØªÙˆÙ†Ø³ÙŠØ©
                "Ø¹Ù†Ø¯ÙŠ Ø±Ù†Ø¯ÙŠ ÙÙˆ", "Ù†Ø­Ø¨ Ù†Ø³Ø¬Ù„ Ù…ÙˆØ¹Ø¯", "Ù…ÙˆØ¹Ø¯ ØºØ¯ÙˆØ©", "Ø±Ù†Ø¯ÙŠ ÙÙˆ Ù…Ø¹ Ø§Ù„Ø·Ø¨ÙŠØ¨",
                # Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©
                "RDV demain", "Je voudrais un rendez-vous", "Prendre RDV",
                "Ajouter un rendez-vous", "RDV mÃ©decin", "RÃ©union demain",
                "Planifier une rÃ©union", "RDV Ã  15h", "Rendez-vous lundi",
                # Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
                "Schedule appointment", "Book a meeting", "Appointment tomorrow",
                "Set up meeting", "Doctor appointment", "Meeting at 3pm",
                "I need an appointment", "Book appointment for Monday",
            ],
            'list_appointments': [
                "Ø¹Ø±Ø¶ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ", "Ø£Ø¸Ù‡Ø± Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯", "Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ", "ÙƒÙ„ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
                "Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯", "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ", "Ø§Ø¹Ø±Ø¶ Ù„ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ", "Ø´ÙˆÙ Ø§Ù„Ù…ÙˆØ§Ø¹ÙŠØ¯",
                "Mes rendez-vous", "Afficher mes RDV", "Voir mes RDV", "Liste RDV",
                "Show appointments", "My appointments", "List all appointments",
            ],
            'check_specific_day': [
                "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ Ø§Ù„ÙŠÙˆÙ…", "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ ØºØ¯Ø§Ù‹", "Ù…Ø§Ø°Ø§ Ù„Ø¯ÙŠ Ø§Ù„ÙŠÙˆÙ…", "Ù…ÙˆØ§Ø¹ÙŠØ¯ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³",
                "Ù…Ø§ Ù‡ÙŠ Ù…ÙˆØ§Ø¹ÙŠØ¯ ØºØ¯Ø§Ù‹", "Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ ÙŠÙˆÙ… 25", "Ù…ÙˆØ§Ø¹ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹",
                "RDV aujourd'hui", "RDV demain", "Mes RDV du lundi",
                "Today's appointments", "Tomorrow's schedule", "Appointments on Monday",
            ],
            'cancel_appointment': [
                "Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ù…ÙˆØ¹Ø¯", "Ø§Ø­Ø°Ù Ø§Ù„Ù…ÙˆØ¹Ø¯", "Ø£Ù„ØºÙŠ Ù…ÙˆØ¹Ø¯ÙŠ", "Ø­Ø°Ù Ø§Ù„Ù…ÙˆØ¹Ø¯ Ø±Ù‚Ù… 5",
                "Ø¥Ù„ØºØ§Ø¡", "Ø§Ù…Ø³Ø­ Ø§Ù„Ù…ÙˆØ¹Ø¯", "Ù„Ø§ Ø£Ø±ÙŠØ¯ Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "Annuler le RDV", "Supprimer rendez-vous", "Annuler",
                "Cancel appointment", "Delete meeting", "Remove appointment",
            ],
            'modify_appointment': [
                "ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¹Ø¯", "ØºÙŠØ± Ø§Ù„Ù…ÙˆØ¹Ø¯", "ØªØ£Ø¬ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¹Ø¯", "ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØªØºÙŠÙŠØ± ÙˆÙ‚Øª Ø§Ù„Ù…ÙˆØ¹Ø¯", "ØªØ¹Ø¯ÙŠÙ„", "ØºÙŠØ± Ø§Ù„Ø³Ø§Ø¹Ø©",
                "Modifier le RDV", "Changer l'heure", "Reporter le RDV",
                "Change appointment", "Reschedule", "Update meeting time",
            ],
            'set_reminder': [
                "Ø°ÙƒØ±Ù†ÙŠ", "ØªØ°ÙƒÙŠØ± Ù‚Ø¨Ù„ 30 Ø¯Ù‚ÙŠÙ‚Ø©", "Ø£Ø±ÙŠØ¯ ØªØ°ÙƒÙŠØ±", "Ø°ÙƒØ±Ù†ÙŠ Ø¨Ø§Ù„Ù…ÙˆØ¹Ø¯",
                "ØªØ°ÙƒÙŠØ± Ù‚Ø¨Ù„ Ø³Ø§Ø¹Ø©", "ÙØ¹Ù„ Ø§Ù„ØªØ°ÙƒÙŠØ±",
                "Rappelle-moi", "Mettre un rappel", "Rappel avant",
                "Remind me", "Set reminder", "Reminder before",
            ],
            'greeting': [
                "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ø£Ù‡Ù„Ø§Ù‹", "ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", "Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±",
                "Ù‡Ø§ÙŠ", "Ù‡Ù„Ø§", "ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ",
                "Bonjour", "Salut", "Bonsoir", "Coucou",
                "Hello", "Hi", "Hey", "Good morning", "Good evening",
            ],
            'thanks': [
                "Ø´ÙƒØ±Ø§Ù‹", "Ø´ÙƒØ±Ø§ Ø¬Ø²ÙŠÙ„Ø§", "Ù…Ø´ÙƒÙˆØ±", "ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ",
                "Merci", "Merci beaucoup", "Merci bien",
                "Thanks", "Thank you", "Thank you so much", "Thanks a lot",
            ],
            'help': [
                "Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ø³Ø§Ø¹Ø¯Ù†ÙŠ", "ÙƒÙŠÙ Ø£Ø³ØªØ®Ø¯Ù…", "Ù…Ø§Ø°Ø§ ØªØ³ØªØ·ÙŠØ¹", "Ø§Ù„Ø£ÙˆØ§Ù…Ø±",
                "ÙƒÙŠÙ Ø£Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯", "Ø´Ø±Ø­", "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…",
                "Aide", "Comment utiliser", "Comment faire",
                "Help", "How to use", "What can you do", "Commands",
            ],
        }
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                SELECT user_message, intent FROM interactions
                WHERE intent IS NOT NULL AND intent != ''
                LIMIT 10000
            ''')
            for row in cursor.fetchall():
                if row[1] in self.INTENT_LABELS:
                    self.samples.append((row[0], row[1]))
            conn.close()
        except:
            pass
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ©
        for intent, examples in training_data.items():
            for text in examples:
                self.samples.append((text, intent))
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.samples)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        text, intent = self.samples[idx]
        
        # Tokenization
        if self.tokenizer:
            encoding = self.tokenizer(
                text,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            input_ids = encoding['input_ids'].squeeze()
            attention_mask = encoding['attention_mask'].squeeze()
        else:
            # fallback Ø¨Ø³ÙŠØ·
            input_ids = torch.zeros(self.max_length, dtype=torch.long)
            attention_mask = torch.ones(self.max_length, dtype=torch.long)
        
        label = self.INTENT_LABELS.index(intent) if intent in self.INTENT_LABELS else len(self.INTENT_LABELS) - 1
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'label': torch.tensor(label, dtype=torch.long)
        }


# ==========================================
# 3. Ù…ØµÙ†Ù BERT Ø§Ù„Ø°ÙƒÙŠ
# ==========================================

class SmartBERTClassifier:
    """Ù…ØµÙ†Ù BERT Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù†ÙˆØ§ÙŠØ§"""
    
    MODEL_OPTIONS = {
        'arabert': 'aubmindlab/bert-base-arabertv2',
        'arabert-large': 'aubmindlab/bert-large-arabertv2',
        'camelbert': 'CAMeL-Lab/bert-base-arabic-camelbert-mix',
        'multilingual': 'bert-base-multilingual-cased'
    }
    
    def __init__(
        self,
        model_name: str = 'arabert',
        model_path: str = "models/bert_intent.pth",
        db_path: str = "agent_data.db"
    ):
        self.model_path = model_path
        self.db_path = db_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.bert_model_name = self.MODEL_OPTIONS.get(model_name, model_name)
        
        self.tokenizer = None
        self.model = None
        self.intent_labels = BERTIntentDataset.INTENT_LABELS
        
        self._initialize()
    
    def _initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("âš ï¸ transformers ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨Ø³ÙŠØ·")
            return
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Tokenizer
            logger.info(f"ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ {self.bert_model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸
            if Path(self.model_path).exists():
                self._load_model()
            else:
                logger.info("â„¹ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ - Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}")
    
    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸"""
        try:
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            self.model = ArabicBERTClassifier(
                model_name=self.bert_model_name,
                num_intents=len(self.intent_labels)
            )
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† {self.model_path}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    
    def train(
        self,
        epochs: int = 5,
        batch_size: int = 16,
        learning_rate: float = 2e-5,
        warmup_steps: int = 100
    ) -> Dict:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if not TRANSFORMERS_AVAILABLE:
            return {'success': False, 'reason': 'transformers not available'}
        
        print("\n" + "="*70)
        print("ğŸ§  ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ AraBERT Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ÙˆØ§ÙŠØ§")
        print("="*70)
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        dataset = BERTIntentDataset(self.tokenizer, db_path=self.db_path)
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)
        
        print(f"\nğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        print(f"   â€¢ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {train_size}")
        print(f"   â€¢ Ø§Ù„ØªØ­Ù‚Ù‚: {val_size}")
        print(f"   â€¢ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {self.bert_model_name}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model = ArabicBERTClassifier(
            model_name=self.bert_model_name,
            num_intents=len(self.intent_labels)
        )
        self.model.to(self.device)
        
        # Optimizer
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        best_val_acc = 0
        history = {'train_loss': [], 'val_acc': []}
        
        print(f"\n{'â”€'*70}")
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            train_loss = 0
            
            for batch in train_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                loss.backward()
                
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # Validation
            self.model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    labels = batch['label'].to(self.device)
                    
                    outputs = self.model(input_ids, attention_mask)
                    _, predicted = torch.max(outputs, 1)
                    
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            val_acc = 100 * correct / total
            
            history['train_loss'].append(train_loss)
            history['val_acc'].append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs} â”‚ Loss: {train_loss:.4f} â”‚ Val Acc: {val_acc:.1f}%", end="")
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self._save_model()
                print(" â­")
            else:
                print()
        
        print(f"{'â”€'*70}")
        print(f"\nğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_val_acc:.1f}%")
        
        return {'success': True, 'best_accuracy': best_val_acc, 'history': history}
    
    def _save_model(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        Path(self.model_path).parent.mkdir(parents=True, exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_name': self.bert_model_name,
            'intent_labels': self.intent_labels,
            'timestamp': datetime.now().isoformat()
        }, self.model_path)
    
    def predict(self, text: str) -> Dict:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ÙŠØ©"""
        if self.model is None or self.tokenizer is None:
            return self._fallback_predict(text)
        
        try:
            self.model.eval()
            
            # Tokenization
            encoding = self.tokenizer(
                text,
                max_length=64,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            input_ids = encoding['input_ids'].to(self.device)
            attention_mask = encoding['attention_mask'].to(self.device)
            
            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask)
                probabilities = torch.softmax(outputs, dim=1)[0]
                confidence, predicted = torch.max(probabilities, 0)
            
            return {
                'intent': self.intent_labels[predicted.item()],
                'confidence': confidence.item(),
                'all_scores': {
                    self.intent_labels[i]: probabilities[i].item()
                    for i in range(len(self.intent_labels))
                },
                'method': 'bert'
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            return self._fallback_predict(text)
    
    def _fallback_predict(self, text: str) -> Dict:
        """ØªÙ†Ø¨Ø¤ Ø§Ø­ØªÙŠØ§Ø·ÙŠ"""
        text_lower = text.lower()
        
        keywords = {
            'add_appointment': ['Ù…ÙˆØ¹Ø¯', 'Ø§Ø¬ØªÙ…Ø§Ø¹', 'rdv', 'rendez', 'appointment', 'meeting', 'schedule'],
            'list_appointments': ['Ø¹Ø±Ø¶', 'Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ', 'afficher', 'mes rdv', 'show', 'list', 'appointments'],
            'cancel_appointment': ['Ø¥Ù„ØºØ§Ø¡', 'Ø­Ø°Ù', 'annuler', 'cancel', 'delete'],
            'greeting': ['Ù…Ø±Ø­Ø¨Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'bonjour', 'salut', 'hello', 'hi'],
            'thanks': ['Ø´ÙƒØ±', 'merci', 'thank'],
            'help': ['Ù…Ø³Ø§Ø¹Ø¯Ø©', 'aide', 'help', 'how']
        }
        
        best_intent = 'unknown'
        best_score = 0
        
        for intent, kws in keywords.items():
            score = sum(1 for kw in kws if kw in text_lower)
            if score > best_score:
                best_score = score
                best_intent = intent
        
        return {
            'intent': best_intent,
            'confidence': min(best_score * 0.3, 0.9),
            'all_scores': {},
            'method': 'fallback'
        }


# ==========================================
# Ø§Ø®ØªØ¨Ø§Ø±
# ==========================================

if __name__ == "__main__":
    print("="*70)
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ AraBERT")
    print("="*70)
    
    classifier = SmartBERTClassifier(model_name='multilingual')
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø¯ÙˆÙ† ØªØ¯Ø±ÙŠØ¨
    test_texts = [
        "Ù…ÙˆØ¹Ø¯ ØºØ¯Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© 3",
        "Ø¹Ø±Ø¶ Ù…ÙˆØ§Ø¹ÙŠØ¯ÙŠ",
        "Ù…Ø±Ø­Ø¨Ø§",
        "RDV demain Ã  15h",
        "Cancel my appointment"
    ]
    
    print("\nğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤:")
    for text in test_texts:
        result = classifier.predict(text)
        print(f"\n'{text}'")
        print(f"  â†’ {result['intent']} ({result['confidence']*100:.0f}%) [{result['method']}]")
